{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> API to get structured output from a string using different LLM providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "from litellm import completion\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def structured_output(\n",
    "    model:str, # Model name, see examples here or LiteLLM docs for complete list\n",
    "    system_prompt:str, # Instructions for LLM to process the input string\n",
    "    response_format:BaseModel, # User-defined Pydantic model to define output \n",
    "    user_prompt:str, # Input string that will be processed\n",
    ") -> BaseModel:\n",
    "    \n",
    "    \"\"\"Get structured output from `model` by combining system and user prompts and making the right API call.\n",
    "    See, [here](https://docs.litellm.ai/docs/completion/json_mode#pass-in-json_schema) for full list of APIs available.\"\"\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        response_format=response_format\n",
    "    )\n",
    "    return response_format.model_validate_json(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control the model that is used for the call by simply adjusting the string in the `model` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"azure/gpt-4o-2024-08-06\" # e.g. openai/gpt-4o-2024-08-06 would use the standard OpenAI\n",
    "system_prompt = \"Extract the event information.\"\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Alice and Bob are going to Carmen's birthday party on 22nd March 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': \"Carmen's Birthday Party\",\n",
       " 'date': '2025-03-22',\n",
       " 'participants': ['Alice', 'Bob']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "r = structured_output(model=model,\n",
    "                      system_prompt=system_prompt,\n",
    "                      response_format=CalendarEvent, #Note this is the class name (without the `()`)\n",
    "                      user_prompt=user_prompt,\n",
    "                 )\n",
    "\n",
    "r.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
